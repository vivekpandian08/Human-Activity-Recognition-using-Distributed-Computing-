{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.install_pypi_package(\"pandas==0.25.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.install_pypi_package(\"s3fs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T01:50:59.577092Z",
     "start_time": "2019-12-09T01:50:41.429786Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import s3fs\n",
    "import time \n",
    "\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages \"org.apache.hadoop:hadoop-aws:2.7.4\" pyspark-shell'\n",
    "sc = SparkContext.getOrCreate()\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "sc._jsc.hadoopConfiguration().set('fs.s3a.access.key', 'AKIA5UF2BJCMTZ4APSDY')\n",
    "sc._jsc.hadoopConfiguration().set('fs.s3a.secret.key', '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')\n",
    "\n",
    "### Load data files from S3\n",
    "\n",
    "file = \"s3a://usf-msds694-group4/merged.txt\"\n",
    "rdd = sc.textFile(file,)\n",
    "rdd = rdd.map(lambda x: x[:-1].split(','))\n",
    "\n",
    "file = \"s3a://usf-msds694-group4/activity_key.txt\"\n",
    "keysrdd = sc.textFile(file,)\n",
    "keysrdd = keysrdd.map(lambda x: x.split('='))\n",
    "\n",
    "### Activities \n",
    "\n",
    "activities = rdd.map(lambda x: x[3])\n",
    "activity_counts = activities.countByValue()\n",
    "\n",
    "activities_df = pd.DataFrame(activity_counts, index=['activity']).T.reset_index()\n",
    "activities_df.columns = ['activity_ID', 'count']\n",
    "\n",
    "\n",
    "key = 'AKIA5UF2BJCMTZ4APSDY'\n",
    "secret = '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'\n",
    "\n",
    "\n",
    "def save_to_s3(df,location):\n",
    "    bytes_to_write = df.to_csv(None).encode()\n",
    "    fs = s3fs.S3FileSystem(key=key, secret=secret)\n",
    "    with fs.open('s3://usf-msds694-group4/'+location, 'wb') as f:\n",
    "        f.write(bytes_to_write)\n",
    "        \n",
    "save_to_s3(activities_df,'activities_df.csv')\n",
    "\n",
    "### Users \n",
    "\n",
    "users = rdd.map(lambda x: x[0])\n",
    "user_counts = users.countByValue()\n",
    "\n",
    "users_df = pd.DataFrame(user_counts, index=['user']).T.reset_index()\n",
    "users_df.columns = ['user_ID', 'count']\n",
    "users_df\n",
    "\n",
    "save_to_s3(users_df,'user_counts.csv')\n",
    "\n",
    "def get_stddev(user):\n",
    "    df = pd.DataFrame()\n",
    "    user0 = rdd.filter(lambda x: x[0]==user)\n",
    "    for i in [5,6,7]:\n",
    "        user0_pairs = user0.map(lambda x: (x[3], float(x[i])))\n",
    "        user0_pairs = user0_pairs.groupByKey()\n",
    "        dictionary = user0_pairs.mapValues(lambda x: np.std(list(x))).collect()\n",
    "        if df.any().any():\n",
    "            df1 = pd.DataFrame(dictionary, columns=['activity_id', 'value'f'{i}'])\n",
    "            df = df.merge(df1, on='activity_id')\n",
    "        else:\n",
    "            df = pd.DataFrame(dictionary, columns=['activity_id', 'value'f'{i}'])\n",
    "            \n",
    "    return df\n",
    "\n",
    "user0 = get_stddev(user='1600')\n",
    "user0['user_id'] = 1600\n",
    "save_to_s3(user0,'user0-stddev.csv')\n",
    "\n",
    "# Activity keys\n",
    "\n",
    "file = \"s3a://usf-msds694-group4/activity_key.txt\"\n",
    "keysrdd = sc.textFile(file,)\n",
    "keysrdd = keysrdd.map(lambda x: x.split('='))\n",
    "\n",
    "activities = pd.DataFrame(keysrdd.collect(), columns = ['name', 'activity_id'])\n",
    "activities['activity_id'] = activities['activity_id'].str.strip() \n",
    "\n",
    "merged = user0.merge(activities, on = 'activity_id', how='left')\n",
    "\n",
    "melted = pd.melt(merged[['value5','value6','value7','name']], id_vars='name')\n",
    "\n",
    "save_to_s3(melted,'melted-user0.csv')\n",
    "\n",
    "# Category wise\n",
    "\n",
    "user0_data = rdd.filter(lambda x: x[0]=='1600').collect()\n",
    "\n",
    "df = pd.DataFrame(user0_data, columns=['user', 'device','sensor_type','activity_id','timestamp','x','y','z'])\n",
    "\n",
    "# save \n",
    "\n",
    "save_to_s3(df, 'user0-overall.csv')\n",
    "\n",
    "df['type'] = df['device']+'-'+df['sensor_type']\n",
    "df['x'] = df['x'].astype(float)\n",
    "df['y'] = df['y'].astype(float)\n",
    "df['z'] = df['z'].astype(float)\n",
    "\n",
    "df = df.merge(activities, on = 'activity_id', how='left')\n",
    "\n",
    "df = df.groupby(['type','activity_id'])[['x','y','z']].std().reset_index()\n",
    "\n",
    "melted = pd.melt(df[['x','y','z','type','activity_id']], id_vars=['type','activity_id'])\n",
    "\n",
    "save_to_s3(melted,'melted_category_wise.csv')\n",
    "\n",
    "act1_data = rdd.filter(lambda x: x[3]=='M').collect()\n",
    "\n",
    "df = pd.DataFrame(act1_data, columns=['user', 'device','sensor_type','activity_id','timestamp','x','y','z'])\n",
    "\n",
    "df['type'] = df['device']+'-'+df['sensor_type']\n",
    "df['x'] = df['x'].astype(float)\n",
    "df['y'] = df['y'].astype(float)\n",
    "df['z'] = df['z'].astype(float)\n",
    "\n",
    "df = df.merge(activities, on = 'activity_id', how='left')\n",
    "\n",
    "df = df.groupby(['type','user'])[['x','y','z']].std().reset_index()\n",
    "\n",
    "melted1 = pd.melt(df[['x','y','z','type','user']], id_vars=['type','user'])\n",
    "\n",
    "save_to_s3(melted1, 'melted-activity-M.csv')\n",
    "\n",
    "act2_data = rdd.filter(lambda x: x[3]=='S').collect()\n",
    "\n",
    "df = pd.DataFrame(act2_data, columns=['user', 'device','sensor_type','activity_id','timestamp','x','y','z'])\n",
    "\n",
    "df['type'] = df['device']+'-'+df['sensor_type']\n",
    "df['x'] = df['x'].astype(float)\n",
    "df['y'] = df['y'].astype(float)\n",
    "df['z'] = df['z'].astype(float)\n",
    "\n",
    "df = df.merge(activities, on = 'activity_id', how='left')\n",
    "\n",
    "df = df.groupby(['type','user'])[['x','y','z']].std().reset_index()\n",
    "\n",
    "melted2 = pd.melt(df[['x','y','z','type','user']], id_vars=['type','user'])\n",
    "\n",
    "save_to_s3(melted2, 'melted-activity-S.csv')\n",
    "\n",
    "# Euclidian plots\n",
    "\n",
    "def get_edist(user,  limit=1000):\n",
    "    user_data = rdd.filter(lambda x: x[0] == user)\n",
    "    data = user_data.filter(lambda x: x[1]=='phone' and x[2]=='gyro')\n",
    "    data = data.map(lambda x: (x[3], (x[5],x[6],x[7])))\n",
    "    data = data.groupByKey()\n",
    "    data = data.map(lambda x: (x[0],list(x[1])[:limit]))\n",
    "    dist = data.flatMapValues(lambda x: x)\n",
    "    dist = dist.mapValues(lambda x: np.sqrt(abs(float(x[0]) + float(x[1]) + float(x[2]))))\n",
    "    df = pd.DataFrame(dist.collect(), columns=['activity_id', 'value'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "data_frame = get_edist('1600',1000)\n",
    "\n",
    "df_merged = data_frame.merge(activities, on = 'activity_id', how='left')\n",
    "\n",
    "save_to_s3(df_merged, 'euclidian.csv')\n",
    "\n",
    "print (\"time taken is\", time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
